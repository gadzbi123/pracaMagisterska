\chapter{Analiza tematu wyszukiwania tekstu}  % Analiza tematu
% 

%\begin{itemize}
%\item Jaki problem chcę (muszę :-) rozwiązać?
%\item Dlaczego rozwiązanie problemu jest ważne?
%\item Jak inni rozwiązują ten problem?
%\item Jakie są zalety i wady tych rozwiązań?
%\end{itemize}

%Odwołania do literatury:
%książek \cite{bib:ksiazka},
%artykułów w czasopismach \cite{bib:artykul},
%materiałów konferencyjnych \cite{bib:konferencja}
%i stron www \cite{bib:internet}.
%
%Równania powinny być numerowane
%wzór
%\begin{align}
%    y = \frac{\partial x}{\partial t}
%\end{align}
%
%
%\begin{itemize}
%\item analiza tematu
%\item wprowadzenie do dziedziny (\english{state of the art}) – sformułowanie problemu, 
%\item poszerzone studia literaturowe, przegląd literatury tematu (należy wskazać źródła wszystkich informacji zawartych w pracy)
%\item opis znanych rozwiązań, algorytmów, osadzenie pracy w kontekście
%\item Tytuł rozdziału jest często zbliżony do tematu pracy. 
%\item Rozdział jest wysycony cytowaniami do literatury \cite{bib:artykul,bib:ksiazka,bib:konferencja}. 
%Cytowanie książki \cite{bib:ksiazka}, artykułu w czasopiśmie \cite{bib:artykul}, artykułu konferencyjnego \cite{bib:konferencja} lub strony internetowej \cite{bib:internet}.
%\end{itemize}

%\begin{Definition}\label{def:1}
%Definicja to zdanie (lub układ zdań) odpowiadające na pytanie o strukturze „co to jest a?”. Definicja normalna jest zdaniem złożonym z 2 członów: definiowanego (łac. definiendum) i definiującego (łac. definiens), połączonych spójnikiem definicyjnym („jest to”, „to tyle, co” itp.). 
%\end{Definition}
%
%\begin{Theorem}[Pitagorasa]\label{t:pitagoras}
%W dowolnym trójkącie prostokątnym suma kwadratów długości przyprostokątnych jest równa kwadratowi długości przeciwprostokątnej tego trójkąta. 
%\end{Theorem}
%
%\begin{Example}[generalizacja]\label{ex:generalizacja}
%Przykładem generalizacji jest para: zwierzę i pies. Pies jest zwierzęciem. Pies jest uszczegółowieniem pojęcia zwierzę. Zwierzę jest uogólnieniem pojęcia pies.
%\end{Example}
\section{Sformułowanie problemu}
Wyszukiwanie tekstu w systemach towarzyszy ludziom od początków istnienia maszyn.
Jednak pierwsze komputery nie posiadały ogromnych ilość pamięci. Z biegiem lat
zaistniała potrzeba odkrycia algorytmów wyszukujących zawartość pamięci. 
Procesor Intel 8008 zaprezentowany w 1972 posiadał jedynie 14-bitową magistralę
adresową, co pozwalało na 16 KB pamięci \cite{bib:internet:Intel8008}. 
Obecna ilość pamięci, którą można otrzymać po założeniu konta z chmury Google'a to 15 GB
\cite{bib:internet:GoogleCloud}, co stanowi milion maksymalnych bloków pamięci w 
urządzeniu ze wspomnianym procesorem Intela. Należy zauważyć, że lokalne zasoby
 pamięci znacznie przewyższają te, które możemy otrzymać za darmo w chmurze.

Zasadniczym problem naszej pracy jest wyszukiwanie zawartości tekstowej
ogromnej ilość plików w różnych formatach. Takie podejście może okazać się 
problematyczne \newline w przypadku plików dźwiękowych, filmowych czy zdjęć
wszelkiego rodzaju. Dodatkowym problemem może okazać się rozwiązanie problemu
przeszukiwania zagnieżdżonych w sobie archiwów.

\section{Dostępne rozwiązania} %state of art

Podjęcie problemu wyszukiwania plików po nazwach oraz zawartości jest bardzo
złożonym i trudnym problemem w sferze programistycznej. Istnieje kilka rozwiązań
tego problemu. Narzędzia takie jak \textbf{find}, \textbf{grep} czy \textbf{ripgrep} 
\cite{bib:internet:ripgrep} pozwalają na wyszukiwanie tekstu. Narzędzia te nie 
są przystosowane do znalezienia pełnej ścieżki pliku dla archiwów, a są jedynie
w stanie określić kontekst znalezienia, bez dokładniej ścieżki pliku.

\subsection{Przykładowe narzędzia dostępne do wykorzystania}

Narzędzie \textbf{find} to znane i popularne narzędzie w systemie z jądrem Linux.
Jest ono bardzo często wykorzystywane do znajdowania plików w systemie po nazwie, 
jednak nie jest przystosowane do znajdowania zawartości plików.

\begin{figure}[htbp]
  \centering
\begin{tcolorbox}[
    colback=white,
    colframe=black,
    boxrule=0.5pt,
    arc=0pt
]
  \begin{minted}{bash}
find /biblioteka -name '*.pdf'
find /biblioteka -path 'książki/*.docx' 
find /biblioteka -name '*.py' -not -path '*/site-packages/*'  
find /biblioteka -maxdepth 2 -size +500k -size -10M           
find /biblioteka -type f -empty -delete 
  \end{minted}
\end{tcolorbox}
\caption{Przykład użycia programu find}
\label{fig:cmd:findExamples}
\end{figure}

Jak pokazano na rysunku (rys. \ref{fig:cmd:findExamples}) te przykładowe komendy find
pozwalają na wyszukanie zawartości folderu biblioteka, a kolejne argumenty 
pozwalają na doprecyzowanie określonych parametrów pliku. Argument \textbf{-name}
pozwala na wyszukanie wszystkich słów odpowiadających nazwie pliku, jednak nie
uwzględniają ścieżki. Argument \textbf{-path} pozwala na wyszukanie plików, 
które odpowiadają podanej ścieżce, a nie jedynie nazwie pliku.

Połączenie argumentów \textbf{-name} oraz argumentu \textbf{-not} z \textbf{-path}
umożliwi wyszukanie plików z rozszerzeniem .py. Dodatkowo odrzucone zostaną 
pliki z paczek pobocznych (ang. \english{site-packages}).

Kolejna komenda wyszuka wszystkie pliki znajdujące się tylko w folderze 
/biblioteka \\ (\textbf{-maxdepth 2}) oraz jednym zagnieżdżonym folderze poniżej.
Dodatkowo pokazane zostaną pliki większe niż 500 KB i mniejsze niż 10 MB.

Ostatnia komenda z rysunku (rys. \ref{fig:cmd:findExamples}) daje możliwość usunięcia 
(\textbf{-delete}) plików (nie folderów) \textbf{-type f}, które są puste (\textbf{-empty}).

Narzędzie to niestety nie ma możliwości przeszukania folderów z pliku
archiwum. Aby dokonać takiego przeszukania, należy wykonać dekompresję danych
przy użyciu tar lub innego podobnego narzędzia. Taki proces nie jest jednak 
prosty w przypadku wielu typów archiwów.

Do przeszukiwania zawartości plików dobrze nadaje się narzędzie grep, które jest
dostępny narzędziem GNU's not unix. Jego działanie jest dość podobne do \textbf{find'a},
gdyż posiada on możliwość wyszukiwania treść w plikach tekstowych. Narzędzie grep również nie 
posiada też możliwość szukania zawartości archiwów oraz nie wspiera wielu 
formatów. Jest możliwość binarnego przeszukania plików, lecz archiwa nie będą 
odczytane w przypadku użycia kompresji.

\begin{figure}[htbp]
  \centering
\begin{tcolorbox}[
    colback=white,
    colframe=black,
    boxrule=0.5pt,
    arc=0pt
]
  \begin{minted}{bash}
grep "szukany-tekst" /biblioteka/plik1.txt 
grep -r "szukany-tekst" /biblioteka 
grep -i "szukany-tekst" /biblioteka/plik1.txt
grep -w "szukany-tekst" /biblioteka/plik1.txt
grep -C 2 "szukany-tekst" plik1.txt
cat /biblioteka/plik1.txt | grep -v "szukany-tekst" 
  \end{minted}
\end{tcolorbox}
\caption{Przykład użycia programu grep}
\label{fig:cmd:grepExamples}
\end{figure}

Na rysunku powyżej przedstawione są przykładowe komendy programu grep 
(rys. \ref{fig:cmd:grepExamples}). Pierwsza komenda wyszukuje tekst 'szukany-tekst' w pliku /biblioteka/plik1.txt.
Kolejna komenda pozwala nam przeszukać rekurencyjnie wszystkie pliki w folderach znajdujące się w 
/biblioteka (\textbf{-r}). Trzecia komenda wyszuka wszystkie wystąpienia 
'szukany-tekst' ignorując przy tym wielkość liter. To pozwoli wyszukać tekst
posiadający treść 'szukany-tekst' w pliku. Dodając argument \textbf{-w}
wyświetla się liczba wiersza, w którym znajdowało się dopasowanie w pliku. 

Przedostatnia komenda pozwala na pokazanie kontekstu znalezionego dopasowania 
(rys. \ref{fig:cmd:grepExamples}). Jeżeli 'szukany-tekst' znajduje się w linii 25, to
program przedstawi nam linie od 23 do 27 z wyszczególnioną treścią, którą 
wyszukiwano. Ostatnia komenda przyjmuje treść innego programu przy pomocy
standardowego wejścia strumieniowego. Ta komenda zwróci nam wszystkie linie,
w których 'szukany-tekst' nie występuje.

Kolejnym narzędziem jest \textbf{ripgrep}. Nie jest on domyślnie instalowany na 
większości systemów Linux, ale pozwala na szybsze wyszukiwanie z powodu
wykorzystania równoległości wątków.

\begin{figure}[htbp]
  \centering
\begin{tcolorbox}[
    colback=white,
    colframe=black,
    boxrule=0.5pt,
    arc=0pt
]
  \begin{minted}{bash}
rg "szukany-tekst" /biblioteka
rg -i "szukany-tekst" /biblioteka
rg -l "szukany-tekst" /biblioteka
rg -c "szukany-tekst" /biblioteka
rg -U "szukany\ntekst" /biblioteka
rg -z "szukany-tekst" /biblioteka/archiwum.zip
  \end{minted}
\end{tcolorbox}
\caption{Przykład użycia programu ripgrep}
\label{fig:cmd:ripgrepExamples}
\end{figure}

Przykładowa komenda pozwalające na wyszukanie frazy 'szukany-tekst' we wszystkich
folderach i podfolderach /biblioteka (rys. \ref{fig:cmd:ripgrepExamples}). Kolejna
komenda pozwala na wyszukanie wszystkich fraz z pominięciem wielkości znaków. 
Podając parametr \textbf{-l} otrzymano nazwy plików bez linii, w których 
wystąpiła fraza. Parametr \textbf{-c} zwróci nam ilość wystąpień tego wyrazu w 
pliku. Parametr \textbf{-U} pozwala na znalezienie wzorca występującego przez 
kilka linii. Parametr \textbf{-z} pozwala na przejrzenie nierozpakowanego 
archiwum. Ostatnia opcja jest bardzo dobra dla plików, które znajdują się w 
zbiorze danych.

\section{Odniesienia do literatury}

Istnieje wiele odniesień do wyszukiwania danych w literaturze. Problem wyszukiwania tekstu 
w dobie internetu, gdzie wiele zasobów przechowywana jest w chmurze,
wymaga stworzenia rozwiązania pozwalającego na szybkie znajdowanie treści.
Indeksowanie stron przyspiesza wyszukiwanie, ale wymaga dodatkowej przestrzeni
pamięci na przechowywanie zaindeksowanych danych \cite{bib:internet:htmlSearchGoogle}.

Ilość wydobywanych i dostarczanych danych poprzez operacje wejścia i wyjścia stanowi duże wyzwanie
oraz wymaga wykorzystania skomplikowanych algorytmów i jest skomplikowanym
problemem. 

Operacje wejścia-wyjścia (I/O) stanowią kluczowy element w architekturze 
komputerowej, wpływając na ogólną wydajność systemu. W artykule 'W skrócie o 
wydajności pamięci' porównano zarządzanie danymi w komputerze do 
organizacji przedmiotów \\ w domu, podkreślając znaczenie efektywnego 
rozmieszczenia i dostępu do zasobów. W kontekście operacji I/O, analogie do 
fizycznych obiektów pomagają zrozumieć, jak istotne jest minimalizowanie 
opóźnień. Kolejnym istotnym aspektem jest zarządzanie pamięcią podręczną (cache) w 
kontekście operacji wejścia / wyjścia. Efektywne wykorzystanie pamięci cache może znacząco 
zmniejszyć opóźnienia dostępu do danych, jednak wymaga to zaawansowanych 
mechanizmów koherencji, zwłaszcza w systemach wieloprocesorowych \cite{bib:internet:IntelMemoryPerformance}. 

Jednym z głównych wyzwań w operacjach I/O jest różnorodność i zmienność 
urządzeń peryferyjnych. Urządzenia te różnią się pod względem szybkości 
działania, protokołów komunikacyjnych oraz sposobu obsługi przez system 
operacyjny. W notatkach dotyczących systemów operacyjnych zwraca się uwagę na 
problem identyfikacji źródła przerwania. Konieczność obsługi współbieżnej 
wielu urządzeń komplikuje proces zarządzania operacjami I/O co opisane
zostało na wykładzie \cite{bib:internet:UrzadzeniaWejsciaWyjscia}. 

Na ten temat w swojej pracy doktorskiej odniósł się też dr Artur Malinowski \cite{bib:internet:ArturMalinowskiIO}. \\
W tej pracy do złożoności systemów odzyskiwania danych (str. 24). Dane 
przetrzymywane w określonym obszarze pamięci L1, L2, L3 znajdują się
bliżej niż na dysku SSD lub HDD \\ i przez to zostaną znacznie szybciej odszukane. 

Artykuł 'Love your cache' \cite{bib:internet:TwojaPamiećPodręczna} przedstawia
kompleksowe podejście do zarządzania 
pamięcią podręczną w kontekście nowoczesnych aplikacji internetowych. Przede 
wszystkim, artykuł podkreśla znaczenie strategicznego podejścia do buforowania 
treści w pamięci podręcznej przeglądarki. Autor wskazuje, że standardy 
dotyczące pamięci podręcznej pochodzące z 1999 roku nie są w pełni dostosowane 
do współczesnych wymagań aplikacji internetowych. Proponowane jest wprowadzenie
bardziej zaawansowanych mechanizmów kontroli, które pozwalają na optymalizację
wydajności przy jednoczesnym zachowaniu aktualności danych.

Istotnym elementem omawianym w artykule jest koncepcja odcisków cyfrowych \\ w
nazwach plików. Polega ona na dodawaniu unikalnych identyfikatorów do nazw 
zasobów, co umożliwia efektywne zarządzanie przechowywanym danymi w pamięci 
podręcznej. Jest to szczególnie ważne w kontekście optymalizacji wydajności.

Podczas konferencji TREC \cite{bib:konferencja:TRECDuplicates} jeden z zespołów 
napotkał problem zduplikowanych danych. Chęć wydobycia danych w celu utworzenia
tekstów wymagało usunięcia duplikacji dokumentów oraz wyborze tej treści,
która posiada najwięcej wartości, co mogłoby być dodatkowym aspektem do wzbogacenia pracy. 

Archiwizacja danych stanowi istotne wyzwanie w różnych dziedzinach, od medycyny 
po zarządzanie dokumentami historycznymi. Analiza systemów archiwizacji obrazów
medycznych oraz procesów ekstrakcji piśmiennictwa wskazuje na kluczowe problemy
i rozwiązania stosowane w tych obszarach. W dokumencie o technicznej formie
proponowanych rozwiązań problemów można w 'Przegląd otwartych rozwiązań systemów 
archiwizacji i komunikacji obrazów medycznych' \cite{bib:internet:ArchiwizacjaDanychMedycznych}.

Archiwa przechowywane mogą też być w różnym stanie. Niektóre dane mogą być
uszkodzone z powodu złego przechowywania, sposobu pozyskanych danych lub 
niepoprawnej implementacji odczytu danych z archiwów. W artykule o
digitalizacji piśmiennictwa można znaleźć analogię do rozwiązywanego problemu \cite{bib:internet:GovDigitalizacjaPiśmiennictwa}.

W książce W. Curtis Prestona można przeczytać o problemie odczytania tych danych w przypadku
uszkodzonych zawartości \cite{bib:ksiazka:ArchiwizacjaIOdzyskiwanie}. Sugeruje
on wiele narzędzi pozwalających na odzyskanie zawartości z archiwów i problem
z radzeniem sobie w przypadkach uszkodzenia danych.

Problemem odczytywania danych mocno skompresowanych, zajmowali się również 
doktorzy z Uniwersytetu w Dublinie \cite{bib:internet:KompresjaDanych}. Odnosili
się oni do skomplikowania odczytu \\ i redundancji w przypadku transportu danych
przez sieć.

\section{Opis poznanych rozwiązań}

\subsection{Algorytm brute force}

Jest wiele algorytmów, które wyszukują tekst. Jednym z takich algorytmów, jest 
algorytm typu brute-force. Polega na sprawdzaniu każdego bajtu, a jego implementacja
jest traktowana jako naiwna. Złożoność czasowa tego rozwiązania wynosi
$O(len(pattern) * len(substring))$, gdzie len(pattern) to długość bloku 
przeszukiwanego, a len(substring) to długość tekstu szukanego. Nie jest wykonana
żadna optymalizacja w tym algorytmie, jak np. przesunięcie się do następnej
 instancji znaku powtórzonego. Przykładową implementację algorytmu można znaleźć
  na rysunku (rys. \ref{fig:code:bruteForceComparison}).

\begin{figure}[htbp]
\centering
\begin{lstlisting}
for i := 0; i<len(pattern); i++{
for j := 0; j<len(substring); j++{
    // compare bytes
}
}
\end{lstlisting}
\caption{Przykład algorytmu brute force}
\label{fig:code:bruteForceComparison}
\end{figure}

\subsection{Interpretacja prefixo-sufiksu}

W algorytmie Morrisa-Pratta (MP) oraz KMP prefikso-sufiks jest kluczowym elementem, który
pozwala na efektywne wyszukiwanie wzorca w tekście.
Prefikso-sufiks to ciąg znaków, który jest jednocześnie prefiksem (początkiem)
i sufiksem (końcem) danego wzorca, ale nie jest całym wzorcem. Innymi słowy, 
jest to wspólna część występująca zarówno na początku, jak i na końcu wzorca.

W algorytmie Morrisa-Pratta (MP) prefikso-sufiks stanowi fundamentalny element 
sposobu wyszukiwania wzorca w tekście. Jest to specyficzna
sekwencja znaków, która występuje jednocześnie na początku (jako prefiks) i na
końcu (jako sufiks) analizowanego wzorca.
Ta właściwość pozwala algorytmowi na znacznie efektywniejsze
działanie \\ w porównaniu do prostego, naiwnego przeszukiwania tekstu.

Działanie prefikso-sufiksu można najlepiej zrozumieć na konkretnym przykładzie.
Weźmy wzorzec 'ABABCA'. W tym przypadku możemy znaleźć różne prefiksy (takie 
jak 'A', 'AB', 'ABA', 'ABAB', 'ABABC') oraz sufiksy ('A', 'CA', 'BCA', 'ABCA',
'BABCA'). Analizując te zbiory, znajdujemy części wspólne, które spełniają 
definicję prefikso-sufiksu. Dla tego konkretnego wzorca, 'AB' jest przykładem 
prefikso-sufiksu, ponieważ występuje zarówno na początku, jak i końcu pewnego 
prefiksu wzorca ('ABAB').

Kluczowym elementem algorytmu MP jest wykorzystanie prefikso-sufiksów do 
konstrukcji tablicy prefiksowej (\textbf{preproc}). Tablica ta przechowuje informacje o 
długościach najdłuższych prefikso-sufiksów dla każdej pozycji we wzorcu. To 
właśnie ta struktura danych pozwala algorytmowi na inteligentne przesuwanie 
wzorca w przypadku wystąpienia niedopasowania, eliminując potrzebę cofania się 
w tekście. Wykorzystanie prefikso-sufiksów w algorytmach MP i KMP prowadzi do znaczącej 
optymalizacji czasowej.

W praktycznych zastosowaniach, zrozumienie koncepcji prefikso-sufiksu jest 
kluczowe w celu zrozumienia działania algorytmów omówionych w tej pracy. 
Właściwości prefikso-sufiksów znajdują również zastosowanie w innych 
algorytmach tekstowych, jak na przykład w algorytmie Aho-Corasick, 
używanym do jednoczesnego wyszukiwania wielu wzorców. Ta uniwersalność 
koncepcji prefikso-sufiksu sprawia, że jest ona jednym z fundamentalnych 
pojęć w dziedzinie algorytmów tekstowych. Algorytm wyszukiwania
wielu wzorców nie będzie omawiany w tej pracy.

\subsection{Algorytm Morisa-Pratta}

\begin{figure}[htbp]
  \centering
  \begin{lstlisting}
preproc := make([]byte, len(substr)+1)
curr = -1
preproc[0] = -1
for i := 1; i <= len(substr); i++ {
  for (curr > -1) && (substr[curr] != substr[i-1]) {
    curr = preproc[curr]
  }
  curr++
  preproc[i] = curr
}
  \end{lstlisting}
  \caption{Przykład uprzedniego procesowania tekstu szukanego}
  \label{fig:code:preprocessMorisPratt}
\end{figure}

Algorytm Morisa-Pratta, jest algorytmem wykorzystującym możliwość procesowania 
łańcucha w tekście, aby dopasować wcześniej odpowiadającą część zbioru
(rys. \ref{fig:code:preprocessMorisPratt}). Polega on na wykorzystaniu faktu
istnienia pasującego prefikso-sufiksu. Pozwala on na pominięcie porównania
znaków, które się powtarzają w łańcuchu poszukiwanym.

Dzięki wykorzystaniu tej zależności można uniknąć cofania się indeksu i. 
Od teraz jako tablicę przechowującą informacje o przesunięciu w przypadku 
błędnego znaku, którą zainicjowano na rysunku (rys. \ref{fig:code:preprocessMorisPratt})
jako tablica \textbf{preproc}.

\begin{figure}[htbp]
    \centering
    \begin{lstlisting}
  res := []int{}
  curr := 0
  found := 0
  for i := 0; i < len(pattern); i++ {
    for (curr > -1) && (substr[curr] != pattern[i]) {
      curr = preproc[curr]
    }
    curr++
    if curr == len(substr) {
      for found < i-curr+1 {
        found++
      }
      res = append(res, found)
      found++
      curr = preproc[curr]
    }
  }
    \end{lstlisting}
    \caption{Przykład procesowania łańcucha poszukiwanego w algorytmie Morisa Pratta}
    \label{fig:code:algoMorisPratt}
  \end{figure}

Tablice \textbf{preproc} należy wypełnić poprzednią wartości tak długo, aż zaistnieje
różnica pomiędzy obecnym, a następnym znakiem tablicy szukanej (tablicy 
\textbf{substr}). W przypadku różnicy obu znaków — zwiększa się wartość zapisywaną 
do tablicy \textbf{preproc} o odległość różnicy znaków. W ten sposób następnym
 razem będzie możliwość pominięcia porównania tych znaków.

W drugim etapie można wykorzystać wcześniej przygotowaną tablicę przemieszczeń 
\textbf{preproc} (rys. \ref{fig:code:algoMorisPratt}), aby obliczyć przesunięcia w przypadku znalezienia 
niepasującego prefiksu. Dzięki temu zwykle 
dłuższy tekst znajdujący się we wzorcu \textbf{pattern} można przeanalizować szybciej
niż w przypadku algorytmu brute-force.

\begin{figure}[htbp]
    \centering
\begin{lstlisting}
func Index(pattern, substr string) int {
  n := len(substr)
  switch {
  case n == 0:
    return 0
  [...]
  case n > len(pattern):
    return -1
  case n <= bytealg.MaxLen: // Zwykle ten przypadek uzyty
    // brute-force kiedy pattern jest krotki
    if len(pattern) <= bytealg.MaxBruteForce /* max == 64 */{
      return bytealg.IndexString(pattern, substr)
    } else { 
    // implementacja BM
    [...]
    }
  }
}
\end{lstlisting}
\caption{Szukanie łańcucha w standardowej bibliotece Golang}
\label{fig:code:golangSearchInsideString}
\end{figure}

W podstawowej bibliotece języka Golang, w pakiecie \textit{strings} istnieje 
implementacja metody \textit{Index()}. Nie jest ona jednak w pełni przedstawiona
w kodzie, natomiast w jej implementacji można zauważyć, że algorytm brute-force
jest wykorzystywany tylko \\ w przypadku, gdy długość wzorca \textbf{pattern} wynosi
mniej lub równo 64 bajty (rys. \ref{fig:code:golangSearchInsideString}).
W przypadku większego tekstu przeszukiwanego wykorzystuje się podobny algorytm 
do implementacji BM. Wartość stała \textbf{bytealg.MaxBruteForce} jest 
zadeklarowana w innym miejscu, ale jej wartość wynosi 64, o czym informuje komentarz.
Do pełnej implementacji zamieszczono odniesienie \cite{bib:online:IndexSearchGolang}.

W implementacji wyszukiwania w Golang, gdy bufor szukany jest mniejszy niż 64 bajty,
obliczenie bufora wcześniejszego procesowania \textbf{BWP} nie jest wykonywane, 
wiec stosuje się algorytm brute-force. Jeżeli wzorzec jest większy niż 64 
bajty to wykonuje się wyszukanie z pomocą \textbf{BWP}.

W Golang, gdy wzorzec jest większy niż
64 znaki, to wykonuje się inny algorytm, który posiada dodatkową walidację w
przypadku odkrycia wyniku fałszywie dodatniego. Algorytm Morisa-Pratta nie
 potrzebuje takiej walidacji.

\subsection{Algorytm Kurta-Morisa-Pratta}

\begin{figure}[htbp]
    \begin{minted}[xleftmargin=0.15\textwidth,xrightmargin=0.2\textwidth,linenos]{diff}
preproc := make([]int, lensubstr+1)
preproc[0] = -1
curr := -1
for i := 1; i <= lensubstr; i++ {
  for (curr > -1) && (substr[curr] != substr[i-1]) {
    curr = preproc[curr]
  }
  curr++
- preproc[i] = curr
+ if (i == lensubstr) || (substr[i] != substr[curr]) {
+   preproc[i] = curr
+ } else {
+   preproc[i] = preproc[curr]
+ }
}
mp.preproc = preproc
    \end{minted}
  \caption{Różnica pomiędzy algorytmami KMP i MP}
  \label{fig:code:KurtMorisPrattVsMorisPratt}
\end{figure}

Kolejny rozpatrywany algorytm jest implementacja, rozszerzająca poprzednią 
implementację przedstawioną w algorytmie Morisa-Pratta. Różnice można zauważyć na podstawie rysunku
(rys. \ref{fig:code:KurtMorisPrattVsMorisPratt}).

Gdy nie osiągnięto długości łańcucha i obecny znak jest równy temu, który 
znajduje się w łańcuchu szukanym (rys. \ref{fig:code:KurtMorisPrattVsMorisPratt}
linijka 10), to można wykonać skok do znaku znajdującego się w tablicy
przygotowanej (rys. \ref{fig:code:KurtMorisPrattVsMorisPratt} linijka 13).
Nie należy sprawdzać kolejnego znak \\ w pętli. Ta różnica powoduje, że algorytm 
wykonuje się szybciej.

Złożoność tego algorytmu wynosi $O(2*{len(pattern)})$, gdzie pattern to długość
tekstu przeszukiwanego. W najbardziej pesymistycznym przypadku, gdy nie 
znaleziono dopasowania, będzie to wymagało ilości operacji powrotu równej długości 
łańcucha szukanego. Powoduje to, że złożoność obliczeniowa sprowadza się do
$O({len(substr)}*{len(pattern)})$, co posiada tę samą złożoność co algorytm naiwny. 

\begin{table}[hbtp]
  \centering
  \begin{tabular}{ |c|c|  } 
    % \hline
    % \multicolumn{2}{|c|}{Przykład wykorzystania algorytmu KMP} \\
    \hline
    wzorzec S & AAAAAABAAAAAABAAAAAAA \\
    \hline
    łańcuch W & AAAAAA \\
    \hline
    liczba cofnięć & 20 \\
    \hline
  \end{tabular}
  \caption{Przykład wykorzystania algorytmu KMP}
  \label{tabela:KMPExampleSlow}
\end{table}

W scenariuszu wykorzystania algorytmu Kurta-Morrisa-Pratt (tab. \ref{tabela:KMPExampleSlow})
można zaobserwować działanie na przykładzie tekstu przeszukiwanego S = 
'AAAAAABAAAAAABAAAAAAA' oraz frazy szukanej W = 'AAAAAA'. W tym przypadku algorytm musi sprawdzić każde
wystąpienie 'A' przed dotarciem do 'B', co jest bardzo nieefektywne, ponieważ
pisany tekst miałby większą różnorodność liter i proces przeszukiwania mógłby 
zacząć się od symbolu B. Sytuacja 
pogarsza się wraz ze wzrostem liczby powtórzeń fragmentu 'AAAAAAB'. Mimo że 
metoda tablicowa \textbf{preproc} działa tu sprawnie (bez potrzeby cofania się), to jej 
jednokrotne wykonanie dla łańcucha szukanego W wykona się podobnie jak w algorytmie
brute-force. Proces ten wyszukiwania często wymaga wielokrotnych powtórzonych przebiegów. 
Wielokrotne przeszukiwanie tekstu S w poszukiwaniu wzorca prowadzi do gorszej
wydajności, gdy zbiór danych zawiera tego typu 
charakterystykę tekstu i wzorca (powtarzające się litery). Naturalny język posiada
rzadziej powtarzające się litery, dlatego algorytm Boyera-Moore'a może stanowić 
optymalne rozwiązanie.

Algorytm KMP wykorzystuje w najgorszym przypadku liniowy przebieg, natomiast
algorytm Boyera-Moore'a w najlepszym przypadku posiada złożoność $O({len(pattern)}+{len(substr)})$, a w 
najgorszym przypadku $O({len(pattern)}*{len(substr)})$.

\subsection{Algorytm Boyera-Moore'a}
\label{sch:algoBoyerMoore}

\begin{figure}[htbp]
    \centering
    \begin{lstlisting}
i := 0
len_str := len(str)
len_substr := len(substr)
for i+len_substr <= len_str {
  for j := len_substr - 1; j >= 0; j-- {
   if str[i+j] != substr[j] {
     if loc := bm.preproc[str[i+j]]; loc == 0 {
       if j == len_substr-1 {
         i += len_substr
       } else {
         i += len_substr - j - 1
       }
     } else {
       n := loc - len_substr + j + 1
       if n <= 0 {
         i++
       } else {
         i += n
       }
     }
     goto loop
   }
  }
  res = append(res, i)
  if v := bm.preproc[str[i+len_substr-1]]; v != 0 {
   i += v
  } else {
   i += len_substr
  }
}
    \end{lstlisting}
    \caption{Główna część wyszukiwania w algorytmie Boyera-Moore'a}
    \label{fig:code:BMmain}
\end{figure}

Algorytm Boyera-Moore'a wprowadza rewolucyjne podejście poprzez skanowanie 
wzorca od prawej do lewej strony, w przeciwieństwie do MP i KMP, które 
analizują tekst od lewej do prawej. Ta fundamentalna różnica pozwala algorytmowi BM na 
znacznie efektywniejsze przeskakiwanie fragmentów tekstu (rys. \ref{fig:code:BMmain}), które na pewno nie 
zawierają wzorca. BM wykorzystuje dwie heurystyki: 'złego znaku' oraz 'dobrego
sufiksu', podczas gdy KMP \\ i MP opierają się na pojedynczej tablicy prefiksowej.

Zaletą algorytmu jest to, że ilość skoków pomiędzy porównaniami jest zwykle 
większa od 1, a gdy istnieje sytuacja, w której litery w łańcuchu szukanym nie
powtarzają się, to można przeskoczyć o długość całego łańcucha szukanego.

\begin{figure}[htbp]
    \centering
    \begin{lstlisting}
if bm.preproc == nil {
  l := len(substr)
  table := make([]int, 256)

  for i := 0; i < l-1; i++ {
    j := substr[i]
    table[j] = l - i - 1
  }
  bm.preproc = table
}
    \end{lstlisting}
    \caption{Kalkulacja BWP w algorytmie Boyera-Moore'a}
    \label{fig:code:BMpreproc}
\end{figure}

W kontekście implementacji algorytm Boyera-Moore'a wymaga utworzenia tokenów dla
każdego znaku w ASCII (rys. \ref{fig:code:BMpreproc}) w buforze wcześniejszego 
procesowania (\textbf{BWP}), co zwiększa złożoność pamięciową w porównaniu do pojedynczej 
tablicy prefiksowej w KMP i MP. Jednak ta dodatkowa pamięć przekłada się na 
możliwość wykonywania większych skoków w tekście. KMP i MP różnią się między 
sobą głównie w sposobie konstrukcji tablicy prefiksowej. KMP wykorzystuje 
bardziej zaawansowaną technikę, która pozwala na uniknięcie niektórych 
niepotrzebnych porównań występujących w MP.

Praktyczny wybór między tymi algorytmami zależy od charakterystyki danych
wejściowych. BM sprawdza się najlepiej w przypadku długich wzorców i tekstów
napisanych w językach naturalnych, gdzie występuje duża różnorodność znaków.
KMP i MP są bardziej przewidywalne w działaniu i mogą być lepszym wyborem dla 
krótkich wzorców lub tekstów o ograniczonym alfabecie jak na przykład sekwencje
DNA.

W algorytmie BM heurystyka 'złego znaku' w algorytmie Boyer-Moore jest jedną \\ z
dwóch głównych heurystyk wykorzystywanych do przyspieszenia procesu wyszukiwania
wzorca w tekście. Jej podstawową zasadą jest przesunięcie wzorca w prawo, gdy 
napotkany zostanie znak w tekście, który nie pasuje do odpowiadającego mu znaku 
we wzorcu. Wielkość tego przesunięcia jest określana na podstawie ostatniego 
wystąpienia "złego znaku" we wzorcu, lub jeśli znak nie występuje we wzorcu,
wzorzec można przesunąć całkowicie poza ten znak.

Działanie tej heurystyki polega na utworzeniu tablicy przesunięć dla wszystkich 
możliwych znaków w alfabecie. Dla każdego znaku występującego we wzorcu 
zapisywana jest pozycja jego ostatniego wystąpienia (licząc od prawej strony 
wzorca). Gdy podczas porównywania tekstu ze wzorcem napotkany zostanie 
niedopasowany znak, algorytm sprawdza jego pozycję w tablicy przesunięć i 
przesuwa wzorzec w prawo o odpowiednią liczbę pozycji, tak aby ten znak mógł 
zostać dopasowany do jego ostatniego wystąpienia we wzorcu.

Heurystyka 'złego znaku' jest szczególnie efektywna w przypadkach, gdy 
wzorzec zawiera znaki, które rzadko występują w przeszukiwanym tekście, ponieważ 
pozwala na wykonanie dużych przesunięć wzorca. Jest ona również bardzo skuteczna
w połączeniu \\ z drugą heurystyką algorytmu Boyer-Moore - heurystyką 'dobrego 
sufiksu'.

Heurystyka 'dobrego sufiksu' w algorytmie Boyer-Moore 
jest drugą z głównych heurystyk tego algorytmu, która współpracuje z heurystyką 
'złego znaku'. Działa ona w sytuacji, gdy podczas porównywania wzorca z tekstem
znajdziemy częściowe dopasowanie — czyli gdy pewien sufiks wzorca został 
dopasowany do tekstu, ale wystąpiło niedopasowanie na wcześniejszej pozycji. 
W takim przypadku, algorytm próbuje wykorzystać informację \\ o tym dopasowanym 
sufiksie, aby wykonać jak największe bezpieczne przesunięcie wzorca.

Implementacja tej heurystyki wymaga wstępnego przetworzenia wzorca i utworzenia 
tablic pomocniczych. Tablica przechowuje informacje o najdalszym 
wystąpieniu każdego sufiksu wzorca w jego wcześniejszej części, pod warunkiem, 
że przed tym wystąpieniem znajduje się inny znak niż we wzorcu. Następnie tablica 
zawiera długości najdłuższych sufiksów, które są jednocześnie prefiksami wzorca.
Te tablice są wykorzystywane do określenia, o ile pozycji można bezpiecznie 
przesunąć wzorzec w prawo, gdy nastąpi częściowe dopasowanie.

Siła heurystyki 'dobrego sufiksu' ujawnia się szczególnie w przypadkach, gdy 
wzorzec zawiera powtarzające się podciągi znaków. Jest ona niezwykle skuteczna
w sytuacjach, gdy długi sufiks został dopasowany, ale wystąpiło niedopasowanie
na wcześniejszej pozycji. W takich przypadkach możliwe jest wykonanie dużych 
przesunięć wzorca, co znacząco przyspiesza proces wyszukiwania. 

Z tego też powodu należy wykonać heurystykę danych, które są analizowane.
Można to zrobić na kilka sposobów i zostanie to przedstawione w następnych rozdziałach.
