
\chapter{Podsumowanie}

%\begin{itemize}
%\item Jaki problem rozwiązałæm?
%\item Jak ten problem rozwiązałæm?
%\item Jakie są dobre i słabe strony mojego rozwiązania?
%\item Czy mogę sformułować jakieś rekomendacje?
%\end{itemize}

% \begin{itemize}
% \item syntetyczny opis wykonanych prac
% \item wnioski
% \item możliwość rozwoju, kontynuacji prac, potencjalne nowe kierunki
% \item Czy cel pracy zrealizowany? 
% \end{itemize}

\section{Opis wykonanych prac}

Przeprowadzone badania koncentrowały się na analizie wydajności algorytmów 
wyszukujących w kontekście przeszukiwania zawartości tekstowej w różnych formatach 
plików, ze szczególnym uwzględnieniem archiwów. Metodyka badawcza obejmowała porównanie
prędkości wykonania algorytmów na mniejszym zbiorze rozpakowanych archiwów oraz 
analizę porównawczą narzędzi pod względem liczby i prędkości wyszukiwań na 
nierozpakowanym zbiorze. Testy przeprowadzono na środowisku Linux.
 
Zbiór danych testowych składał się z różnorodnych formatów plików o łącznym 
rozmiarze około 15 GB, z dominującymi formatami PDF, ZIP i RAR. W trakcie badań
uwzględniono problematykę różnych formatów plików, co pozwoliło na kompleksową
ocenę wydajności algorytmów w rzeczywistych warunkach. W ramach analizy
algorytmów porównano implementacje Morris-Pratt, Knuth-Morris-Pratt i Boyer-Moore.
Szczególną uwagę poświęcono wpływowi optymalizacji bufora wstępnego przetwarzania
na wydajność oraz znaczeniu ponownego wykorzystania buforów w kolejnych wyszukiwaniach.
Badania wykazały istotny wpływ tych optymalizacji na ogólną wydajność wyszukiwania.

Istotnym elementem badań było porównanie autorskiego narzędzia \textbf{gsearch} z 
popularnymi rozwiązaniami takimi jak \textbf{ugrep}, \textbf{zgrep} i \textbf{ripgrep}. Analiza objęła 
różnice w funkcjonalności, szczególnie w kontekście obsługi archiwów, oraz wpływ
pamięci podręcznej na wydajność wyszukiwania. Wykazano, że każde z narzędzi ma
swoje specyficzne zalety i ograniczenia, zwłaszcza w kontekście obsługi
zagnieżdżonych archiwów i różnych formatów plików. Profilowanie wydajności 
pozwoliło zidentyfikować główne wąskie gardła w procesie wyszukiwania. Wykazano,
że operacje I/O, w tym odczyt z dysku i ekstrakcja archiwów, stanowią największe
obciążenie, podczas gdy sam algorytm wyszukiwania zajmuje relatywnie małą część
całkowitego czasu wykonania. Autorskie rozwiązanie \textbf{gsearch}, choć ustępuje
wydajnością \textbf{ripgrep} w przypadku prostych wyszukiwań, oferuje unikalne 
funkcjonalności, szczególnie w kontekście obsługi złożonych struktur archiwów.

\section{Dowiedzenie celów i wnioski}

Algorytm Boyera-Moore'a wykonał się szybciej, dlatego że zachowywał więcej 
informacji o łańcuchu szukanym, co pokrywa się z hipotezą badań.

Potwierdza się również hipoteza, iż im większy ciąg znaków wyszukujemy, tym
rozwiązania mogą szybciej przeszukać zawartość.

Wynik rozwiązania (\textbf{gsearch}) posiada większe odchylenie standardowe od
wyników z narzędzia \textbf{ripgrep}, co może świadczyć o tym, że Garbage
Collector wpływa na regularność otrzymanych wyników w wymiarze czasu.

Wnioskiem jest to, że czas odczytywania danych z dysku w przypadku braku
istnienia ich w pamięci podręcznej (cache), znacznie wpływa na szybkość 
uzyskanych wyników. 

\section{Możliwości rozwoju}

Program można usprawnić w celu wyszukiwania większej ilości zawartości przy
pomocy dodatkowej implementacji dla transkrypcji. Pozwoliłaby ona na wyszukanie
treści w plikach audio, a istnieje wiele narzędzi, które są darmowe i już
na to pozwalają.

Kolejnym elementem, który można rozważyć w celu kontynuacji pracy, byłoby 
wykorzystanie OCR (ang. \english{Optical Character Recognition}). Zaimplementowanie
takiego rozwiązania pozwoli pozyskać treść ze zdjęć oraz pdfów, które składają 
się ze zdjęć i tekstowych skanów treści.

W celu uzyskania lepszych rezultatów można, zamiast wykorzystywać gotową 
bibliotekę to stworzyć bibliotekę dekompresującą, wszystkich brakujących i nie
poprawnie działających formatów archiwów. Jest to jednak dość wymagające zadanie
oraz możliwość testowania tego rozwiązania jest ograniczona do tego zbioru archiwów. 
Należy postawić pytanie, jak bardzo plik może być uszkodzony, żeby można było
odczytać z niego dane.

Kolejnym usprawnieniem dla programu byłoby wprowadzenie pamięci podręcznej (ang. \english{caching}).
Takie rozwiązanie pozwoliłoby na zapamiętanie plików, które już kiedyś dekompresowano.
Pliki skompresowane zwykle nie mogą być zapisane, wiec po pierwszej dekompresji
można zachować tylko znalezione wystąpienia. Należy jednak przechować 
informacje o tym, czy suma kontrolna (ang. \english{hash}) archiwum się nie 
zmieniła.

Dodatkowo warto by pozwolić programowi działać na większej ilości wątków. To 
wymaga dodatkowej synchronizacji pomiędzy odczytem archiwów, natomiast
Golang jest bardzo łatwym językiem do wprowadzania takich zmian.

Kolejnym kierunkiem do rozwoju jest wykorzystanie wyrażeń regularnych podczas
wyszukiwania zawartości. Taka implementacja na pewno spowolni algorytm 
wyszukujący, natomiast da więcej rezultatów.